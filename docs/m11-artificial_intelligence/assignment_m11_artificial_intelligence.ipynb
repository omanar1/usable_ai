{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hairy-homework",
   "metadata": {},
   "source": [
    "# Homework 11 - LDA and ChatBot\n",
    "In this assignment, we will be applying LDA and building a simple chatbot using the provided datasets.\n",
    "\n",
    "Complete the missing parts in this guide.\n",
    "\n",
    "### Step 1: Load Data\n",
    "You can load the data from the provided TSV file using `pandas`.\n",
    "\n",
    "### Step 2: Preprocess\n",
    " - Clean the data by removing stop-words, punctuations, emoticons etc..\n",
    "\n",
    "### Step 3: Apply LDA\n",
    " - Find the topics in the dataset using LDA (Latent Dirichlet Allocation).\n",
    " - Describe the topics found in the dataset.\n",
    "\n",
    "### Step 4: Create a simple ChatBot\n",
    " - Use nltk to create a simple chatbot that can respond to user queries based on similarity of sentences in the dataset and the user input. \n",
    "\n",
    "## Dataset Overview\n",
    "The dataset obtained originally from https://archive.ics.uci.edu/dataset/331/sentiment+labelled+sentences contains sentences labeled with sentiment. Each sentence is associated with a sentiment label (positive or negative). The dataset is split into three parts, each containing sentences from different sources: Amazon, Yelp, and IMDb.\n",
    "Score is either 1 (for positive) or 0 (for negative)\t\n",
    "\n",
    "\n",
    "## Submission Guidelines\n",
    "\n",
    "- Submit your completed notebook as a HTML export, or a PDF file.\n",
    "\n",
    "To export to HTML, if you are on Jupyter, select `File` > `Export Notebook As` > `HTML`.\n",
    "\n",
    "If you are on VSCode, you can use the `Jupyter: Export to HTML` command.\n",
    " - Open the command palette (Ctrl+Shift+P or Cmd+Shift+P on Mac).\n",
    "     - Search for `Jupyter: Export to HTML`.\n",
    "     - Save the HTML file to your computer and submit it via Canvas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import string\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-feedback",
   "metadata": {},
   "source": [
    "Like last week, we need to load the data from the TSV file. This time we will only use the amazon reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../Datasets/amazon_cells_labelled.tsv\", sep=\"\\t\") # adjust the path as needed\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-share",
   "metadata": {},
   "source": [
    "With our dataframe made, we now need to clean it before analyzing. Apply the `remove_punctuation()` and `remove_stopwords()` functions on our dataset to clean it. Save the cleaned data to a new column named `cleaned_sentence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment the line below if you need to download the stopwords\n",
    "#nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('','', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(text)\n",
    "\n",
    "df[_] = df['sentence'].apply(remove_punctuation).apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-aviation",
   "metadata": {},
   "source": [
    "We need to adjust our data slightly before using LDA. In the cell below, use the `CountVectorizer()` function. Then, use `fit_transform()` with `df['cleaned_sentence']` as a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = _(max_features = 5000, max_df=.15) # Your Code Here\n",
    "X = vect._(df[_]) # Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-gross",
   "metadata": {},
   "source": [
    "Using the `LatenDirichletAllocation()` function below, we want to pass it 10 components. You can adjust the max iterations for your local setup, or leave it as 25 if unsure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lda = _(n_components=10, learning_method=\"batch\", max_iter=25, random_state=0) # Your Code Here\n",
    "document_topics = lda._(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-answer",
   "metadata": {},
   "source": [
    "And finally' let's see the results! Call the `print_topics()` function below, passing in `feature_names` and `sorting`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vect.get_feature_names_out())\n",
    "\n",
    "#slide 27\n",
    "def print_topics(topics, feature_names, sorting, topics_per_chunk, n_words):\n",
    "    for i in range(0, len(topics), topics_per_chunk):\n",
    "        these_topics = topics[i: i + topics_per_chunk]\n",
    "        len_this_chunk = len(topics)\n",
    "        \n",
    "        print(*these_topics)\n",
    "        print(\"----------------------\")\n",
    "\n",
    "\n",
    "        for i in range(n_words):\n",
    "            try:\n",
    "                print(*feature_names[sorting[these_topics, i]])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "            \n",
    "            \n",
    "_(topics=range(10), feature_names=feature_names, sorting=sorting, topics_per_chunk=5, n_words=10) # Your Code Here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf78a15",
   "metadata": {},
   "source": [
    "## ChatBot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aadd67",
   "metadata": {},
   "source": [
    "Let's build a simple chatbot using rules and sentence similarity. In this particular case we will use the TFIDF vectorizer to convert our sentences into vectors. Note that modern chatbots now use deep learning models, but this is a good exercise to understand the basics of how chatbots can work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d086bf5f",
   "metadata": {},
   "source": [
    "We will perform a little bit more preprocessing this time. In addition to removal of punctuation and stopwords, we will also lemmatize the words in our dataset. Lemmatization is the process of reducing a word to its base or root form. For example, \"running\" becomes \"run\". This helps in reducing the dimensionality of our dataset and improves the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47171040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "sent_tokens = df[_].str.lower().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414055db",
   "metadata": {},
   "source": [
    "Let's define our lemmatization function first. You need to use the `WordNetLemmatizer` from the `nltk` library. Make sure to download the WordNet data if you haven't already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be31ff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')    # sentence/token splitter\n",
    "nltk.download('wordnet')  # for lemmatization\n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "\n",
    "# remove punctuation, tokenize, and lemmatize in one call\n",
    "remove_punct = dict((ord(p), None) for p in string.punctuation)\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(t) for t in tokens]\n",
    "\n",
    "def LemNormalize(text): # Normalize text by removing punctuation, tokenizing, and lemmatizing\n",
    "    return LemTokens(word_tokenize(text.lower().translate(remove_punct)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7cca76",
   "metadata": {},
   "source": [
    "Let's define some greeting inputs and responses. These will be used to match user inputs with predefined responses. Add your own greetings and responses to the lists below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeee6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS  = {\"hello\",\"hi\", ...} # Add more greetings as needed\n",
    "GREETING_RESPONSES = [\"hi\",\"hey\", \"sup\", ...]\n",
    "# Add more greeting responses as needed\n",
    "\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0475f2",
   "metadata": {},
   "source": [
    "We need now to define a response function that will take user input and return a response based on  the most similar entries in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a006b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise    import cosine_similarity\n",
    "\n",
    "def response(user_response):\n",
    "    robo_response = \"\"\n",
    "    # temporarily add user query so TF-IDF matrix includes it\n",
    "    sent_tokens.append(user_response)\n",
    "    tfidf     = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english').fit_transform(sent_tokens)\n",
    "    vals      = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx       = vals.argsort()[0][-2]    # second-highest similarity\n",
    "    flat      = vals.flatten()\n",
    "    flat.sort()\n",
    "    sim_score = flat[-2]\n",
    "    sent_tokens.pop()                    # remove user query\n",
    "\n",
    "    if sim_score == 0: # if no similar sentences found\n",
    "        robo_response = \"I’m sorry, I don’t understand.\"\n",
    "    else:\n",
    "        robo_response = sent_tokens[idx]\n",
    "    return robo_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e48e974",
   "metadata": {},
   "source": [
    "Now Let's create the interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9c0345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    # Create a s\n",
    "    print(\"ROBO: My name is Robo. Ask me anything about product reviews. Type 'bye' to exit.\")\n",
    "    while True:\n",
    "        user_input = input(\"YOU: \").lower().strip()\n",
    "        if user_input == 'bye': # exit condition. Important!\n",
    "            print(\"ROBO: Goodbye! Take care.\")\n",
    "            break\n",
    "        if user_input in ('thanks','thank you'):\n",
    "            print(\"ROBO: You’re welcome!\")\n",
    "            break\n",
    "        # greeting?\n",
    "        greet = greeting(user_input)\n",
    "        if greet:\n",
    "            print(f\"ROBO: {_}\") # YOUR Code Here\n",
    "        else:\n",
    "            print(f\"ROBO: {response(_)}\") # YOUR Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa4d2c7",
   "metadata": {},
   "source": [
    "Now test it. See if you can find a few sentences that match the reviews in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c04581",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b17f793",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
