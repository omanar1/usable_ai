{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hairy-homework",
   "metadata": {},
   "source": [
    "# Homework 11 - LDA and ChatBot\n",
    "In this assignment, we will be applying LDA and building a simple chatbot using the provided datasets.\n",
    "\n",
    "Complete the missing parts in this guide.\n",
    "\n",
    "### Step 1: Load Data\n",
    "You can load the data from the provided TSV file using `pandas`.\n",
    "\n",
    "### Step 2: Preprocess\n",
    " - Clean the data by removing stop-words, punctuations, emoticons etc..\n",
    "\n",
    "### Step 3: Apply LDA\n",
    " - Find the topics in the dataset using LDA (Latent Dirichlet Allocation).\n",
    " - Describe the topics found in the dataset.\n",
    "\n",
    "### Step 4: Create a simple ChatBot\n",
    " - Use nltk to create a simple chatbot that can respond to user queries based on similarity of sentences in the dataset and the user input. \n",
    "\n",
    "## Dataset Overview\n",
    "The dataset obtained originally from https://archive.ics.uci.edu/dataset/331/sentiment+labelled+sentences contains sentences labeled with sentiment. Each sentence is associated with a sentiment label (positive or negative). The dataset is split into three parts, each containing sentences from different sources: Amazon, Yelp, and IMDb.\n",
    "Score is either 1 (for positive) or 0 (for negative)\t\n",
    "\n",
    "\n",
    "## Submission Guidelines\n",
    "\n",
    "- Submit your completed notebook as a HTML export, or a PDF file.\n",
    "\n",
    "To export to HTML, if you are on Jupyter, select `File` > `Export Notebook As` > `HTML`.\n",
    "\n",
    "If you are on VSCode, you can use the `Jupyter: Export to HTML` command.\n",
    " - Open the command palette (Ctrl+Shift+P or Cmd+Shift+P on Mac).\n",
    "     - Search for `Jupyter: Export to HTML`.\n",
    "     - Save the HTML file to your computer and submit it via Canvas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "miniature-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import string\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-feedback",
   "metadata": {},
   "source": [
    "Like last week, we need to load the data from the TSV file. This time we will only use the amazon reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "important-complexity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  score\n",
       "0  So there is no way for me to plug it in here i...      0\n",
       "1                        Good case, Excellent value.      1\n",
       "2                             Great for the jawbone.      1\n",
       "3  Tied to charger for conversations lasting more...      0\n",
       "4                                  The mic is great.      1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Datasets/amazon_cells_labelled.tsv\", sep=\"\\t\") # adjust the path as needed\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-share",
   "metadata": {},
   "source": [
    "With our dataframe made, we now need to clean it before analyzing. Apply the `remove_punctuation()` and `remove_stopwords()` functions on our dataset to clean it. Save the cleaned data to a new column named `cleaned_sentence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "knowing-murray",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>score</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "      <td>way plug us unless go converter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "      <td>good case excellent value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "      <td>great jawbone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "      <td>tied charger conversations lasting 45 minutesm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "      <td>mic great</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  score  \\\n",
       "0  So there is no way for me to plug it in here i...      0   \n",
       "1                        Good case, Excellent value.      1   \n",
       "2                             Great for the jawbone.      1   \n",
       "3  Tied to charger for conversations lasting more...      0   \n",
       "4                                  The mic is great.      1   \n",
       "\n",
       "                                    cleaned_sentence  \n",
       "0                    way plug us unless go converter  \n",
       "1                          good case excellent value  \n",
       "2                                      great jawbone  \n",
       "3  tied charger conversations lasting 45 minutesm...  \n",
       "4                                          mic great  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uncomment the line below if you need to download the stopwords\n",
    "#nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('','', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(text)\n",
    "\n",
    "df['cleaned_sentence'] = df['sentence'].apply(remove_punctuation).apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-aviation",
   "metadata": {},
   "source": [
    "We need to adjust our data slightly before using LDA. In the cell below, use the `CountVectorizer()` function. Then, use `fit_transform()` with `df['cleaned_sentence']` as a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "lasting-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(max_features = 5000, max_df=.15) # Your Code Here\n",
    "X = vect.fit_transform(df['cleaned_sentence']) # Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-gross",
   "metadata": {},
   "source": [
    "Using the `LatenDirichletAllocation()` function below, we want to pass it 10 components. You can adjust the max iterations for your local setup, or leave it as 25 if unsure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "quantitative-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, learning_method=\"batch\", max_iter=25, random_state=0) # Your Code Here\n",
    "document_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-answer",
   "metadata": {},
   "source": [
    "And finally' let's see the results! Call the `print_topics()` function below, passing in `feature_names` and `sorting`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "consolidated-aging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4\n",
      "----------------------\n",
      "works sound great recommend battery\n",
      "great really use would good\n",
      "happy quality reception service horrible\n",
      "easy good make customer software\n",
      "battery headset car one life\n",
      "junk product like highly also\n",
      "use bad new ear product\n",
      "piece well working right cell\n",
      "cheap bluetooth product stay never\n",
      "item service light terrible nice\n",
      "5 6 7 8 9\n",
      "----------------------\n",
      "use ive dont work great\n",
      "design best money like excellent\n",
      "good ever waste disappointed quality\n",
      "think ear buy charger price\n",
      "money one product well good\n",
      "work worst everything time love\n",
      "case bought good worked product\n",
      "awesome long within im headset\n",
      "problems purchase broke new poor\n",
      "volume well one motorola sound\n"
     ]
    }
   ],
   "source": [
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vect.get_feature_names_out())\n",
    "\n",
    "#slide 27\n",
    "def print_topics(topics, feature_names, sorting, topics_per_chunk, n_words):\n",
    "    for i in range(0, len(topics), topics_per_chunk):\n",
    "        these_topics = topics[i: i + topics_per_chunk]\n",
    "        len_this_chunk = len(topics)\n",
    "        \n",
    "        print(*these_topics)\n",
    "        print(\"----------------------\")\n",
    "\n",
    "\n",
    "        for i in range(n_words):\n",
    "            try:\n",
    "                print(*feature_names[sorting[these_topics, i]])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "            \n",
    "            \n",
    "print_topics(topics=range(10), feature_names=feature_names, sorting=sorting, topics_per_chunk=5, n_words=10) # Your Code Here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf78a15",
   "metadata": {},
   "source": [
    "## ChatBot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aadd67",
   "metadata": {},
   "source": [
    "Let's build a simple chatbot using rules and sentence similarity. In this particular case we will use the TFIDF vectorizer to convert our sentences into vectors. Note that modern chatbots now use deep learning models, but this is a good exercise to understand the basics of how chatbots can work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d086bf5f",
   "metadata": {},
   "source": [
    "We will perform a little bit more preprocessing this time. In addition to removal of punctuation and stopwords, we will also lemmatize the words in our dataset. Lemmatization is the process of reducing a word to its base or root form. For example, \"running\" becomes \"run\". This helps in reducing the dimensionality of our dataset and improves the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47171040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "sent_tokens = df['sentence'].str.lower().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414055db",
   "metadata": {},
   "source": [
    "Let's define our lemmatization function first. You need to use the `WordNetLemmatizer` from the `nltk` library. Make sure to download the WordNet data if you haven't already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be31ff3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\EUSRIOM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\EUSRIOM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')    # sentence/token splitter\n",
    "nltk.download('wordnet')  # for lemmatization\n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "\n",
    "# remove punctuation, tokenize, and lemmatize in one call\n",
    "remove_punct = dict((ord(p), None) for p in string.punctuation)\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(t) for t in tokens]\n",
    "\n",
    "def LemNormalize(text): # Normalize text by removing punctuation, tokenizing, and lemmatizing\n",
    "    return LemTokens(word_tokenize(text.lower().translate(remove_punct)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7cca76",
   "metadata": {},
   "source": [
    "Let's define some greeting inputs and responses. These will be used to match user inputs with predefined responses. Add your own greetings and responses to the lists below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "edeee6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS  = {\"hello\",\"hi\", \"hey\", \"howdy\", \"good morning\", \"good afternoon\", \"good evening\" } # Add more greetings as needed\n",
    "GREETING_RESPONSES = [\"hi\",\"hey\", \"sup\", \"howdy\", \"hello\", \"hi there\", \"good to see you\", \"hello there\"]\n",
    "# Add more greeting responses as needed\n",
    "\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0475f2",
   "metadata": {},
   "source": [
    "We need now to define a response function that will take user input and return a response based on  the most similar entries in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a006b682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\EUSRIOM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise    import cosine_similarity\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def response(user_response):\n",
    "    robo_response = \"\"\n",
    "    # temporarily add user query so TF-IDF matrix includes it\n",
    "    sent_tokens.append(user_response)\n",
    "    tfidf     = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english').fit_transform(sent_tokens)\n",
    "    vals      = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx       = vals.argsort()[0][-2]    # second-highest similarity\n",
    "    flat      = vals.flatten()\n",
    "    flat.sort()\n",
    "    sim_score = flat[-2]\n",
    "    sent_tokens.pop()                    # remove user query\n",
    "\n",
    "    if sim_score == 0: # if no similar sentences found\n",
    "        robo_response = \"I’m sorry, I don’t understand.\"\n",
    "    else:\n",
    "        robo_response = sent_tokens[idx]\n",
    "    return robo_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e48e974",
   "metadata": {},
   "source": [
    "Now Let's create the interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d9c0345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    # Create a s\n",
    "    print(\"ROBO: My name is Robo. Ask me anything about product reviews. Type 'bye' to exit.\")\n",
    "    while True:\n",
    "        user_input = input(\"YOU: \").lower().strip()\n",
    "        if user_input == 'bye': # exit condition. Important!\n",
    "            print(\"ROBO: Goodbye! Take care.\")\n",
    "            break\n",
    "        if user_input in ('thanks','thank you'):\n",
    "            print(\"ROBO: You’re welcome!\")\n",
    "            break\n",
    "        # greeting?\n",
    "        greet = greeting(user_input)\n",
    "        if greet:\n",
    "            print(f\"ROBO: {greet}\") # YOUR Code Here\n",
    "        else:\n",
    "            print(f\"ROBO: {response(user_input)}\") # YOUR Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa4d2c7",
   "metadata": {},
   "source": [
    "Now test it. See if you can find a few sentences that match the reviews in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c04581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: My name is Robo. Ask me anything about product reviews. Type 'bye' to exit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EUSRIOM\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\EUSRIOM\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: I’m sorry, I don’t understand.\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d499d95c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b17f793",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
